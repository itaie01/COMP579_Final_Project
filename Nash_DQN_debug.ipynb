{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjqosHq8qhPm",
        "outputId": "1e7c3c10-41e1-46a8-e631-272a1ef46e5d"
      },
      "outputs": [],
      "source": [
        "# !pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2pUqHRn9qmOn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from pettingzoo.mpe import simple_speaker_listener_v3, simple_reference_v2, simple_world_comm_v2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "76vm-s5MqoOV"
      },
      "outputs": [],
      "source": [
        "#Global Variables\n",
        "DISCOUNT = 0.99\n",
        "MINIBATCH_SIZE = 16  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "\n",
        "# Environments\n",
        "EPISODES = 200\n",
        "MAX_CYCLES = 25\n",
        "\n",
        "REPLAY_MEMORY_SIZE = int(EPISODES * MAX_CYCLES / 5)  # How many last steps to keep for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = int(REPLAY_MEMORY_SIZE / 5 /2)  # Minimum number of steps in a memory to start training\n",
        "\n",
        "# Exploration settings\n",
        "EPSILON = 0.1  # decaying epsilon\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Global Variables\n",
        "DISCOUNT = 0.99\n",
        "MINIBATCH_SIZE = 16  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "\n",
        "# Environments\n",
        "EPISODES = 200\n",
        "MAX_CYCLES = 25\n",
        "\n",
        "REPLAY_MEMORY_SIZE = int(EPISODES * MAX_CYCLES / 5)  # How many last steps to keep for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = int(REPLAY_MEMORY_SIZE / 5 /2)  # Minimum number of steps in a memory to start training\n",
        "\n",
        "# Exploration settings\n",
        "EPSILON = 0.1  # decaying epsilon\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Global Variables\n",
        "DISCOUNT = 0.99\n",
        "MINIBATCH_SIZE = 16  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "\n",
        "# Environments\n",
        "EPISODES = 200\n",
        "MAX_CYCLES = 25\n",
        "\n",
        "REPLAY_MEMORY_SIZE = int(EPISODES * MAX_CYCLES / 5)  # How many last steps to keep for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = int(REPLAY_MEMORY_SIZE / 5 /2)  # Minimum number of steps in a memory to start training\n",
        "\n",
        "# Exploration settings\n",
        "EPSILON = 0.1  # decaying epsilon\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EbNQmJRkqqJa"
      },
      "outputs": [],
      "source": [
        "def eps_greedy_act_selection(epsilon, action_space_size, q_values):\n",
        "    if np.random.random() < epsilon:\n",
        "        # randomly choose one action\n",
        "        return np.random.randint(0, action_space_size)\n",
        "    else:\n",
        "        # all q values\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "def normalization(X):\n",
        "        mu = np.mean(X, axis=0)\n",
        "        sigma = np.std(X, axis=0)\n",
        "    \n",
        "        # Normalize the features using the mean and standard deviation\n",
        "        X_norm = (X - mu) / sigma\n",
        "\n",
        "        return X_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZwvPbhGC5hs"
      },
      "source": [
        "## Individual DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x-q7n715qr7Z"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, input_layer_size, action_space_size):\n",
        "        # Main model which we use to train\n",
        "        self.model = self.create_model(input_layer_size, action_space_size)\n",
        "\n",
        "        # Target network to make sure the updating is stable\n",
        "        self.target_model = self.create_model(input_layer_size, action_space_size)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # The array to keep the memory for the last n steps for training\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "        # Count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "\n",
        "    def create_model(self, input_layer_size, action_space_size):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation='relu', input_shape=(input_layer_size,)))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(action_space_size, activation = 'linear'))\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        print(sys.getsizeof(transition))\n",
        "\n",
        "        # transition = (s, a, r, s', done)\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "\n",
        "    def train(self, terminal_state):\n",
        "        # Start training only if enough transition samples has been collected in the memory\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Get a minibatch from memory replay table\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # Get the current states and their corresponding q values for each sample in the minibatch\n",
        "        current_states = np.array([transition[0] for transition in minibatch])\n",
        "        # current_states = StandardScaler().fit_transform(current_states)\n",
        "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
        "\n",
        "        # Get the next states their corresponding q values for each sample in the minibatch\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
        "        # new_current_states = StandardScaler().fit_transform(new_current_states)\n",
        "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "            \n",
        "            # Update Q value for the given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # Prepare training data\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False, verbose=0)\n",
        "\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "        \n",
        "        # update target network with weights of main network if condition satisfied\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape), verbose=0)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aqbgpbXAqyKH"
      },
      "outputs": [],
      "source": [
        "AGENT_NAMES = ['speaker_0', 'listener_0']\n",
        "AGENT_INFOS = {\n",
        "    name:\n",
        "        {\n",
        "            \"state_space_size\": 3 if name == 'speaker_0' else 11, \n",
        "            \"action_space_size\": 3 if name == 'speaker_0' else 5 \n",
        "        } for name in AGENT_NAMES\n",
        "}\n",
        "\n",
        "def dqn_sl(epsilon, num_episode, max_cycles, env):\n",
        "    # initialize DQNAgents for the listener / speaker\n",
        "    all_dqn_agents = {name: DQNAgent(AGENT_INFOS[name][\"state_space_size\"], \n",
        "                                     AGENT_INFOS[name][\"action_space_size\"]) for name in AGENT_NAMES}\n",
        "    \n",
        "    all_rewards = {agent_name:[] for agent_name in AGENT_NAMES}\n",
        "    \n",
        "    for episode in tqdm(range(1, num_episode + 1), ascii=True, unit='episodes'):\n",
        "    # for episode in range(num_episode):\n",
        "        # initialize environment & reward\n",
        "        env.reset()\n",
        "        episode_agent_reward = {agent_name:0 for agent_name in AGENT_NAMES}\n",
        "        \n",
        "        for agent in env.agent_iter():\n",
        "            # do not do step if terminated \n",
        "            if env.truncations[agent] == True or env.terminations[agent] == True:\n",
        "                    env.step(None)\n",
        "                    continue    \n",
        "            # storing size of action and state (metadata about agent)\n",
        "            agent_info = AGENT_INFOS[agent]\n",
        "            # store the corresponding dqn agent (not the game agent, the agent that does dqn stuff)\n",
        "            dqn_agent = all_dqn_agents[agent]\n",
        "            # actual agent living in environment\n",
        "            game_agent_cur_state = env.observe(agent)\n",
        "            \n",
        "            game_agent_q_val = dqn_agent.get_qs(game_agent_cur_state)\n",
        "            game_agent_action_size = agent_info[\"action_space_size\"]\n",
        "            action_taken = eps_greedy_act_selection(epsilon, game_agent_action_size, game_agent_q_val)\n",
        "            \n",
        "            # take the action choosen from eps greedy\n",
        "            env.step(action_taken)\n",
        "            \n",
        "            # get reward and accumulate it\n",
        "            _, R, termination, truncation, info = env.last()\n",
        "            done = termination or truncation\n",
        "            episode_agent_reward[agent] += R\n",
        "            \n",
        "            # get next state S'\n",
        "            game_agent_next_state = env.observe(agent)\n",
        "            \n",
        "            # update replay memory, and train if we have enough replay memory\n",
        "            \n",
        "            dqn_agent.update_replay_memory((game_agent_cur_state, action_taken, R, game_agent_next_state, done))\n",
        "            dqn_agent.train(done)\n",
        "        \n",
        "        # store the total rewards for last game play in one episode\n",
        "        for name in AGENT_NAMES:\n",
        "            all_rewards[name].append(episode_agent_reward[name]/max_cycles)\n",
        "\n",
        "        if epsilon > MIN_EPSILON:\n",
        "            epsilon *= EPSILON_DECAY\n",
        "            epsilon = max(MIN_EPSILON, epsilon)\n",
        "             \n",
        "    return all_rewards\n",
        "\n",
        "# Returns a initialized weight vector with 0\n",
        "def rand_sl(num_episode, max_cycles,  env):\n",
        "    all_rewards = {agent_name:[] for agent_name in AGENT_NAMES}\n",
        "    \n",
        "    for i in range(num_episode):\n",
        "        env.reset()\n",
        "        episode_agent_reward = {agent_name:0 for agent_name in AGENT_NAMES}\n",
        "        for agent in env.agent_iter():\n",
        "            if env.truncations[agent] == True or env.terminations[agent] == True:\n",
        "                env.step(None)\n",
        "                continue\n",
        "            \n",
        "            A = env.action_space(agent).sample()\n",
        "\n",
        "            env.step(A)\n",
        "            _, R, termination, truncation, info = env.last()\n",
        "            episode_agent_reward[agent] += R\n",
        "\n",
        "            if termination or truncation:\n",
        "                continue\n",
        "            \n",
        "        # store the total rewards for last game play in one episode\n",
        "        for name in AGENT_NAMES:\n",
        "            all_rewards[name].append(episode_agent_reward[name]/max_cycles)\n",
        "    return all_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jjcgnSj9Jyw"
      },
      "source": [
        "## Nash DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TKkXzi4gDFl0"
      },
      "outputs": [],
      "source": [
        "LAMBDA_NASH = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MNxYWRVl9LzA"
      },
      "outputs": [],
      "source": [
        "class NashDQNAgent:\n",
        "    def __init__(self, agent_idx, input_layer_size, action_space_size, num_agents):\n",
        "        self.agent_idx = agent_idx #Assume that idx 0 is speaker and 1 is listener\n",
        "        self.input_layer_size = input_layer_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.num_agents = num_agents\n",
        "        \n",
        "        # Create the main model used for training\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        # Create the target model used for updating the main model\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # Initialize the replay memory\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "        # Counter for updating the target model\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation='relu', input_shape=(self.input_layer_size,)))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(self.action_space_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        # Add the transition to the replay memory buffer\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    def get_qs(self, state):\n",
        "        # the model takes in a full state (14 = 11 + 3)\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape), verbose=0)[0]\n",
        "\n",
        "    def get_nash_q(self, state):\n",
        "        # Compute the Nash Q values for both agents given the current state\n",
        "        agent_qs = self.model.predict(state.reshape(-1, *state.shape), verbose=0)[0]\n",
        "        other_agent_qs = np.zeros((self.action_space_size,))\n",
        "        \n",
        "        for i in range(self.num_agents):\n",
        "            if i != self.agent_idx:\n",
        "                other_agent_qs += self.model.predict(state.reshape(-1, *state.shape), batch_size=1, verbose=0)[0]\n",
        "\n",
        "        nash_qs = np.zeros((self.action_space_size,))\n",
        "\n",
        "        for action in range(self.action_space_size):\n",
        "            nash_qs[action] = agent_qs[action] + (other_agent_qs.max() - other_agent_qs[action]) / (self.num_agents - 1)\n",
        "\n",
        "        return nash_qs\n",
        "\n",
        "\n",
        "    def train(self, terminal_state):\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Sample a minibatch from the replay memory\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # Compute the current Q values for the agent's actions in the minibatch\n",
        "        full_states = np.array([transition[0] for transition in minibatch])\n",
        "        current_states = full_states\n",
        "        if self.agent_idx == 0:\n",
        "            current_states[:, -11:] = 0\n",
        "        else:\n",
        "            current_states[:, :3] = 0\n",
        "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
        "\n",
        "        # Compute the next Q values for the agent's actions in the minibatch\n",
        "        full_new_current_states = np.array([transition[3] for transition in minibatch])\n",
        "        new_current_states = full_new_current_states\n",
        "        if self.agent_idx == 0:\n",
        "            new_current_states[:, -11:] = 0\n",
        "        else:\n",
        "            new_current_states[:, :3] = 0\n",
        "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for index, (full_current_state, actions, rewards, _, all_done_states) in enumerate(minibatch):\n",
        "            # Compute the Nash Q value for the current state\n",
        "            # nash_qs = self.get_nash_q(full_current_state)\n",
        "            done = all_done_states[self.agent_idx]\n",
        "            reward = rewards[self.agent_idx]\n",
        "            action = actions[self.agent_idx]\n",
        "\n",
        "            if not done:\n",
        "                # Compute the target Q value using the Nash Q values\n",
        "                tmp_full_current_state = full_current_state[:,None].reshape((1,14))\n",
        "                max_future_q = np.max(self.target_model.predict(tmp_full_current_state, verbose=0))\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "                # new_q = reward + DISCOUNT * max_future_q\n",
        "                # nash_q_diff = abs(current_qs_list[index] - nash_qs) * LAMBDA_NASH\n",
        "                # new_q = (1 - LAMBDA_NASH) * new_q + nash_q_diff\n",
        "            else:\n",
        "                new_q = reward\n",
        "            \n",
        "            # Update Q value for the given state and action\n",
        "            \n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # Prepare training data\n",
        "            X.append(full_current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        # Fit the model using the minibatch\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False, verbose=0)\n",
        "\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "        \n",
        "        # Update target network with weights of main network if condition satisfied\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def nash_dqn_sl(epsilon, num_episode, max_cycles, env):\n",
        "    AGENT_NAMES = ['speaker_0', 'listener_0']\n",
        "    AGENT_INFOS = {name: {\"agent_idx\": 0 if name == 'speaker_0' else 1,\n",
        "                          \"action_space_size\": 3 if name == 'speaker_0' else 5\n",
        "                          } for name in AGENT_NAMES}\n",
        "    NASH_DQN_AGENTS = {name: NashDQNAgent(AGENT_INFOS[name][\"agent_idx\"], 14,\n",
        "                                          AGENT_INFOS[name][\"action_space_size\"], 2) for name in AGENT_NAMES}\n",
        "    UPDATE_COUNTER = 0\n",
        "    ALL_REWARDS = {agent_name:[] for agent_name in AGENT_NAMES}\n",
        "    \n",
        "    for episode in tqdm(range(1, num_episode + 1), ascii=True, unit='episodes'):\n",
        "        # for episode in range(num_episode):\n",
        "        # initialize environment & reward\n",
        "        env.reset()\n",
        "        episode_agent_reward = {agent_name:0 for agent_name in AGENT_NAMES}\n",
        "        full_state = []\n",
        "        full_next_state = []\n",
        "        rewards = []\n",
        "        actions = [] \n",
        "        all_done_states = []\n",
        "        \n",
        "        for agent in env.agent_iter():\n",
        "            # storing size of action and state (metadata about agent)\n",
        "            agent_info = AGENT_INFOS[agent]\n",
        "            # do not do step if terminated \n",
        "            if env.truncations[agent] == True or env.terminations[agent] == True:\n",
        "                env.step(None)\n",
        "                continue\n",
        "\n",
        "            # store the corresponding dqn agent (not the game agent, the agent that does dqn stuff)\n",
        "            nash_dqn_agent = NASH_DQN_AGENTS[agent]\n",
        "            # actual agent living in environment\n",
        "            game_agent_cur_state = env.observe(agent)\n",
        "            \n",
        "            # append to form a full state WITHOUT PADDING\n",
        "            full_state.append(game_agent_cur_state)\n",
        "\n",
        "            # need to perform a padding to current agent state\n",
        "            #speaker, append 11 0's at the end\n",
        "            if agent_info[\"agent_idx\"] == 0: \n",
        "                masked_game_agent_cur_state = np.concatenate((np.array(game_agent_cur_state), np.zeros(11)))\n",
        "            else:\n",
        "                masked_game_agent_cur_state = np.concatenate((np.zeros(3), game_agent_cur_state))\n",
        "                \n",
        "            game_agent_q_val = nash_dqn_agent.get_qs(masked_game_agent_cur_state)\n",
        "            game_agent_action_size = agent_info[\"action_space_size\"]\n",
        "            action_taken = eps_greedy_act_selection(epsilon, game_agent_action_size, game_agent_q_val)\n",
        "            actions.append(action_taken)\n",
        "            # take the action choosen from eps greedy\n",
        "            env.step(action_taken)\n",
        "            \n",
        "            # get reward and accumulate it\n",
        "            _, R, termination, truncation, info = env.last()\n",
        "            episode_agent_reward[agent] += R\n",
        "            rewards.append(R)\n",
        "            done = termination or truncation\n",
        "            all_done_states.append(done)\n",
        "            \n",
        "            game_agent_next_state = env.observe(agent)\n",
        "            full_next_state.append(game_agent_next_state)\n",
        "            UPDATE_COUNTER += 1\n",
        "            \n",
        "            \n",
        "            if UPDATE_COUNTER == 2:\n",
        "                # for plotting\n",
        "                for idx in range(len(AGENT_NAMES)):\n",
        "                    name = AGENT_NAMES[idx]\n",
        "                    combined_full_state = np.concatenate(full_state)\n",
        "                    combined_full_next_state = np.concatenate(full_next_state)\n",
        "                    trainsition = (combined_full_state, actions, rewards, combined_full_next_state, all_done_states)\n",
        "                    nash_dqn_agent = NASH_DQN_AGENTS[name]\n",
        "                    nash_dqn_agent.update_replay_memory(trainsition)\n",
        "                    done_state = all_done_states[idx]\n",
        "                    \n",
        "                    # with open(\"test.log\", \"a\") as f:\n",
        "                    #     f.write(f\"episode: {episode}, \")\n",
        "                    #     f.write(f\"replay memory size: {len(nash_dqn_agent.replay_memory)}\\n\")\n",
        "                    \n",
        "                    nash_dqn_agent.train(done_state)\n",
        "                \n",
        "                # clear it\n",
        "                full_state = []\n",
        "                full_next_state = []\n",
        "                rewards = []\n",
        "                actions = [] \n",
        "                all_done_states = []\n",
        "                UPDATE_COUNTER = 0\n",
        "        # store the total rewards for last game play in one episode\n",
        "        for name in AGENT_NAMES:\n",
        "            ALL_REWARDS[name].append(episode_agent_reward[name]/max_cycles)\n",
        "        \n",
        "        if epsilon > MIN_EPSILON:\n",
        "            epsilon *= EPSILON_DECAY\n",
        "            epsilon = max(MIN_EPSILON, epsilon)\n",
        "             \n",
        "    return ALL_REWARDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1 Pro\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/200 [00:00<?, ?episodes/s]2023-04-24 16:04:50.868583: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "  3%|3         | 6/200 [03:14<2:55:31, 54.28s/episodes]"
          ]
        }
      ],
      "source": [
        "env = simple_speaker_listener_v3.env(max_cycles=MAX_CYCLES, continuous_actions=False)\n",
        "nash_dqn_rewards = nash_dqn_sl(EPSILON, EPISODES, MAX_CYCLES, env)\n",
        "\n",
        "with open(f\"nash_dqn_data_ep{EPISODES}_cycles_{MAX_CYCLES}.json\", \"w\") as f:\n",
        "    # store the experiement result\n",
        "    json.dump(nash_dqn_rewards, f)\n",
        "\n",
        "plt.plot(range(EPISODES), nash_dqn_rewards['speaker_0'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(EPISODES), nash_dqn_rewards['listener_0'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(np.mean(nash_dqn_rewards['listener_0'][:50]))\n",
        "print(np.mean(nash_dqn_rewards['listener_0'][-50:]))\n",
        "print(np.mean(nash_dqn_rewards['listener_0']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(np.mean(nash_dqn_rewards['speaker_0'][:50]))\n",
        "print(np.mean(nash_dqn_rewards['speaker_0'][-50:]))\n",
        "print(np.mean(nash_dqn_rewards['speaker_0']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMzjnGAp5IPGlMnX4t5PyMT",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "0822a54a5dcf558552277e0e3d2d71b79c858f6dfa2317b682e5eae99f200a1e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
