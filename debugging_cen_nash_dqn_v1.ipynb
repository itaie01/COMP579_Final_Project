{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhongjie-wu/579project/blob/main/debugging_cen_nash_dqn_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "CzimeIpXIJj9"
      },
      "outputs": [],
      "source": [
        "# ! pip install pettingzoo[mpe]\n",
        "# ! pip install tf-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "FvzuHks_IJj-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from pettingzoo.mpe import simple_speaker_listener_v3, simple_reference_v2, simple_world_comm_v2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tf_agents.utils.nest_utils as nest_utils\n",
        "from tf_agents.specs import tensor_spec\n",
        "# from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "3AfwAQTbIJj_"
      },
      "outputs": [],
      "source": [
        "#Global Variables\n",
        "DISCOUNT = 0.99\n",
        "MINIBATCH_SIZE = 32  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "\n",
        "# Environments\n",
        "EPISODES = 500\n",
        "MAX_CYCLES = 25\n",
        "\n",
        "REPLAY_MEMORY_SIZE = 100  # How many last steps to keep for model training\n",
        "CRITIC_MIN_REPLAY_MEMORY_SIZE = 50\n",
        "AGENT_MIN_REPLAY_MEMORY_SIZE = 80  # Minimum number of steps in a memory to start training\n",
        "\n",
        "# Exploration settings\n",
        "EPSILON = 0.1  # decaying epsilon\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZpvYLiIJkA"
      },
      "source": [
        "## Nash DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw9WPWIPIJkB"
      },
      "source": [
        "#### Replay Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRhmr7esIJkB"
      },
      "source": [
        "The reason why this is implemented as a seperate class is because the data in this memory will be shared across the listener, speaker and centralized DQN network. Hence, saving into one object saves memory at runtime (i.e. no multiple appending)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "dqAvPng6IJkB"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_len = REPLAY_MEMORY_SIZE):\n",
        "        self.data_spec =  (\n",
        "            tf.TensorSpec([3,], tf.float32, 'S_sp'),\n",
        "            tf.TensorSpec([11,], tf.float32, 'S_ls'),\n",
        "            tf.TensorSpec([], tf.float32, 'A_sp'),\n",
        "            tf.TensorSpec([], tf.float32, 'A_ls'),\n",
        "            tf.TensorSpec([], tf.float32, 'R_sp'),\n",
        "            tf.TensorSpec([], tf.float32, 'R_ls'),\n",
        "            tf.TensorSpec([3,], tf.float32, 'S_next_sp'),\n",
        "            tf.TensorSpec([11,], tf.float32, 'S_next_ls'),\n",
        "            tf.TensorSpec([], tf.bool, 'done_sp'),\n",
        "            tf.TensorSpec([], tf.bool, 'done_ls'),\n",
        "        )\n",
        "        \n",
        "        # self.data_spec =  (\n",
        "        #     tf.TensorSpec((3,), np.float32, 'S_sp'),\n",
        "        #     tf.TensorSpec((11,), np.float32, 'S_ls'),\n",
        "        #     tf.TensorSpec((3,), np.float32, 'A_sp'),\n",
        "        #     tf.TensorSpec((5,), np.float32, 'A_ls'),\n",
        "        #     tf.TensorSpec((1,), np.float32, 'R_sp'),\n",
        "        #     tf.TensorSpec((1,), np.float32, 'R_ls'),\n",
        "        #     tf.TensorSpec((3,), np.float32, 'S_next_sp'),\n",
        "        #     tf.TensorSpec((11,),np.float32, 'S_next_ls'),\n",
        "        #     tf.TensorSpec((1,), np.bool_, 'done_sp'),\n",
        "        #     tf.TensorSpec((1,), np.bool_, 'done_ls'),\n",
        "        # )\n",
        "\n",
        "        self.size = 0\n",
        "\n",
        "        # self.replay_memory = deque(maxlen=max_len)\n",
        "        self.replay_memory = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "                                data_spec=self.data_spec,\n",
        "                                max_length=max_len,\n",
        "                                batch_size=MINIBATCH_SIZE\n",
        "                                )\n",
        "\n",
        "    def add_sample(self, sample):\n",
        "        # sp = speaker, ls = listener, the format of a sample is:\n",
        "        # (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls)\n",
        "        # self.replay_memory.append(sample)\n",
        "        self.replay_memory.add_batch(sample)\n",
        "        if self.size < REPLAY_MEMORY_SIZE:\n",
        "            self.size += 1\n",
        "\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.size\n",
        "    \n",
        "    def sample_minibatch(self, minibatch_size = MINIBATCH_SIZE):\n",
        "        # return random.sample(self.replay_memory, minibatch_size)\n",
        "        return self.replay_memory.get_next(sample_batch_size=minibatch_size)\n",
        "    \n",
        "    def get_mem(self):\n",
        "        return self.replay_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eic4u9L4IJkC"
      },
      "source": [
        "#### Nash DQN agent (For each game env agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "Hwfl7ryjIJkC"
      },
      "outputs": [],
      "source": [
        "class NashDQNAgent:\n",
        "    def __init__(self, input_layer_size, action_space_size, ReplayMemoryObject, is_speaker, critic):\n",
        "        # Main model which we use to train\n",
        "        self.model = self.create_model(input_layer_size, action_space_size)\n",
        "\n",
        "        # Target network to make sure the updating is stable\n",
        "        self.target_model = self.create_model(input_layer_size, action_space_size)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # The array to keep the memory for the last n steps for training\n",
        "        self.replay_memory = ReplayMemoryObject\n",
        "\n",
        "        # Count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "        # Check if this agent is a speaker, if not then listener\n",
        "        self.is_speaker = is_speaker\n",
        "\n",
        "        # Add the critic -- a centalized network to give Q values for joint actions by inputing joint observations\n",
        "        self.critic = critic\n",
        "\n",
        "    def create_model(self, input_layer_size, action_space_size):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation='relu', input_shape=(input_layer_size,)))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(action_space_size, activation = 'linear'))\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, terminal_state):\n",
        "        # Start training only if enough transition samples has been collected in the memory\n",
        "        if self.replay_memory.get_size() < AGENT_MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Get a minibatch from memory replay table\n",
        "        minibatch = self.replay_memory.sample_minibatch(minibatch_size = MINIBATCH_SIZE)\n",
        "\n",
        "        # Get the current states and their corresponding q values for each sample in the minibatch\n",
        "        current_states = np.array([transition[0] for transition in minibatch]) if self.is_speaker else np.array([transition[1] for transition in minibatch])\n",
        "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        # for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "        for index, (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls) in enumerate(minibatch):\n",
        "            done = done_sp if self.is_speaker else done_ls\n",
        "            reward = R_sp if self.is_speaker else R_ls\n",
        "            action = A_sp if self.is_speaker else A_ls\n",
        "            current_state = S_sp if self.is_speaker else S_ls\n",
        "\n",
        "            if not done:\n",
        "                # Calculate Nash Q using the centralized network\n",
        "                joint_observation = np.concatenate((S_next_sp, S_next_ls), axis=None)\n",
        "                joint_q_vals = self.critic.get_qs(joint_observation)\n",
        "                nash_q = np.max(joint_q_vals)\n",
        "                # Nash update\n",
        "                new_q = reward + DISCOUNT * nash_q \n",
        "            else:\n",
        "                new_q = reward\n",
        "            \n",
        "            # Update Q value for the given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # Prepare training data\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False, verbose=0)\n",
        "\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "        \n",
        "        # update target network with weights of main network if condition satisfied\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape), verbose=0)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGS-men9IJkD"
      },
      "source": [
        "#### Critic DQN Agent (For providing the Nash Q value of joint states/actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEwaaK1DIJkD"
      },
      "source": [
        "This DQN Agent serves as the critic for our Nash DQN algorithm. It takes in the joint states observed by the two agents, and then output an array of Q_valus that corresponds to each combination of agents'action. In a fully collaborative settings, we know that for a given state, the joint actions that lead to the maximal Q value is the nash equilibria move and this maximal Q value is the Nash_Q value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "PN37lUvzIJkE"
      },
      "outputs": [],
      "source": [
        "class JointCritic:\n",
        "    def __init__(self, input_layer_size, action_space_size, ReplayMemoryObject):\n",
        "        # Main model which we use to train\n",
        "        self.model = self.create_model(input_layer_size, action_space_size)\n",
        "\n",
        "        # Target network to make sure the updating is stable\n",
        "        self.target_model = self.create_model(input_layer_size, action_space_size)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # The array to keep the memory for the last n steps for training\n",
        "        self.replay_memory = ReplayMemoryObject\n",
        "\n",
        "        # Count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "\n",
        "    def create_model(self, input_layer_size, action_space_size):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation='relu', input_shape=(input_layer_size,)))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(action_space_size, activation = 'linear'))\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    def train(self, terminal_state):\n",
        "        # Start training only if enough transition samples has been collected in the memory\n",
        "        if self.replay_memory.get_size() < CRITIC_MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Get a minibatch from memory replay table\n",
        "        minibatch = self.replay_memory.sample_minibatch(minibatch_size = MINIBATCH_SIZE)\n",
        "\n",
        "        print(minibatch[8])\n",
        "        print(type(minibatch[8]))\n",
        "\n",
        "        # Get the current states and their corresponding q values for each sample in the minibatch\n",
        "        current_states = np.array([np.concatenate((transition[0], transition[1]), axis=None) for transition in minibatch])\n",
        "        # current_states = StandardScaler().fit_transform(current_states)\n",
        "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
        "\n",
        "        # Get the next states their corresponding q values for each sample in the minibatch\n",
        "        new_current_states = np.array([np.concatenate((transition[6], transition[7]), axis=None) for transition in minibatch])\n",
        "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        # for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "        for index, (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls) in enumerate(minibatch):\n",
        "            done = done_sp or done_ls\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = R_sp + R_ls + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = R_sp + R_ls\n",
        "            \n",
        "            # Update Q value for the given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            action_idx = np.ravel_multi_index((A_sp, A_ls), dims=(3, 5))\n",
        "            current_qs[action_idx] = new_q\n",
        "\n",
        "            # Prepare training data\n",
        "            X.append(current_states[index])\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False, verbose=0)\n",
        "\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "        \n",
        "        # update target network with weights of main network if condition satisfied\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape), verbose=0)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UumH_hvIJkE"
      },
      "source": [
        "#### Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "nYmhinFmIJkE"
      },
      "outputs": [],
      "source": [
        "def eps_greedy_act_selection(epsilon, action_space_size, q_values):\n",
        "    if np.random.random() < epsilon:\n",
        "        # randomly choose one action\n",
        "        return np.random.randint(0, action_space_size)\n",
        "    else:\n",
        "        # all q values\n",
        "        return np.argmax(q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "hD6Ro-CFIJkF",
        "outputId": "d8e9d957-3dba-40e7-d2b4-001752273185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/500 [00:00<?, ?episodes/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-2d13593da673>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mUPDATE_COUNTER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mS_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_ls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_ls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_ls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_next_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_next_ls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mreplay_buff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Clean up after adding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-106a4df84d5a>\u001b[0m in \u001b[0;36madd_sample\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# self.replay_memory.append(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mREPLAY_MEMORY_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/replay_buffers/replay_buffer.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mAdds\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py\u001b[0m in \u001b[0;36m_add_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0mwrite_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rows_for_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mwrite_id_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mwrite_data_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_id_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_data_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/replay_buffers/table.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, rows, values, slots)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mflattened_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mflattened_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     write_ops = [\n\u001b[0m\u001b[1;32m    129\u001b[0m         tf.compat.v1.scatter_update(self._slot2storage_map[slot], rows,\n\u001b[1;32m    130\u001b[0m                                     value).op\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/replay_buffers/table.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mflattened_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     write_ops = [\n\u001b[0;32m--> 129\u001b[0;31m         tf.compat.v1.scatter_update(self._slot2storage_map[slot], rows,\n\u001b[0m\u001b[1;32m    130\u001b[0m                                     value).op\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_slots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36mscatter_update\u001b[0;34m(ref, indices, updates, use_locking, name)\u001b[0m\n\u001b[1;32m    429\u001b[0m     return gen_state_ops.scatter_update(ref, indices, updates,\n\u001b[1;32m    430\u001b[0m                                         use_locking=use_locking, name=name)\n\u001b[0;32m--> 431\u001b[0;31m   return ref._lazy_read(gen_resource_variable_ops.resource_scatter_update(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    432\u001b[0m       \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m       name=name))\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_scatter_update\u001b[0;34m(resource, indices, updates, name)\u001b[0m\n\u001b[1;32m   1172\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ResourceScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [3], indices.shape [32], params.shape [3200,3] [Op:ResourceScatterUpdate]"
          ]
        }
      ],
      "source": [
        "AGENT_NAMES = ['speaker_0', 'listener_0']\n",
        "AGENT_INFOS = {name: {\"agent_idx\": 0 if name == 'speaker_0' else 1,\n",
        "                        \"action_space_size\": 3 if name == 'speaker_0' else 5,\n",
        "                        \"input_layer_size\": 3 if name == 'speaker_0' else 11,\n",
        "                        \"is_speaker\": True if name == 'speaker_0' else False\n",
        "                        } for name in AGENT_NAMES}\n",
        "\n",
        "UPDATE_COUNTER = 0\n",
        "ALL_REWARDS = {agent_name:[] for agent_name in AGENT_NAMES}\n",
        "epsilon = EPSILON\n",
        "\n",
        "# Create a replay buffer\n",
        "replay_buff = ReplayMemory(max_len = REPLAY_MEMORY_SIZE)\n",
        "\n",
        "# Create the critic DQN Agent\n",
        "critic_dqn = JointCritic(input_layer_size=14, action_space_size=15, ReplayMemoryObject=replay_buff)\n",
        "\n",
        "# Create the Nash DQN Agents\n",
        "NASH_DQN_AGENTS = {name: NashDQNAgent(input_layer_size=AGENT_INFOS[name][\"input_layer_size\"],\n",
        "                                      action_space_size=AGENT_INFOS[name][\"action_space_size\"],\n",
        "                                      ReplayMemoryObject=replay_buff,\n",
        "                                      is_speaker=AGENT_INFOS[name][\"is_speaker\"],\n",
        "                                      critic=critic_dqn) for name in AGENT_NAMES}\n",
        "\n",
        "# Create the environment\n",
        "env = simple_speaker_listener_v3.env(max_cycles=MAX_CYCLES, continuous_actions=False)\n",
        "\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
        "    # Reset the environment and the reward for this new episode\n",
        "    env.reset()\n",
        "    episode_agent_reward = {agent_name:0 for agent_name in AGENT_NAMES}\n",
        "\n",
        "    # Initialize the SARSD for collecting and building dataset later\n",
        "    S_sp=S_ls=A_sp=A_ls=R_sp=R_ls=S_next_sp=S_next_ls=done_sp=done_ls=None\n",
        "\n",
        "    for agent in env.agent_iter():\n",
        "        if env.truncations[agent] == True or env.terminations[agent] == True:\n",
        "                env.step(None)\n",
        "                continue\n",
        "        \n",
        "        # Get the current agent\n",
        "        nash_dqn_agent = NASH_DQN_AGENTS[agent]\n",
        "        # Observe the current state\n",
        "        state_curr = env.observe(agent)\n",
        "        # Get the Q values for each action of the curr state\n",
        "        q_state_curr = nash_dqn_agent.get_qs(state_curr)\n",
        "        # Choose and take an action\n",
        "        action = eps_greedy_act_selection(epsilon, AGENT_INFOS[agent][\"action_space_size\"], q_state_curr)\n",
        "        env.step(action)\n",
        "        # Get reward and accumulate it\n",
        "        _, reward, termination, truncation, info = env.last()\n",
        "        episode_agent_reward[agent] += reward\n",
        "        # Observe the next state\n",
        "        state_next = env.observe(agent)\n",
        "\n",
        "        # Update the variables\n",
        "        if nash_dqn_agent.is_speaker:\n",
        "            S_sp = state_curr\n",
        "            A_sp = action\n",
        "            R_sp = reward\n",
        "            S_next_sp = state_next\n",
        "            done_sp = termination or truncation\n",
        "        else:\n",
        "            S_ls = state_curr\n",
        "            A_ls = action\n",
        "            R_ls = reward\n",
        "            S_next_ls = state_next\n",
        "            done_ls = termination or truncation\n",
        "\n",
        "        # Increase the update counter\n",
        "        UPDATE_COUNTER += 1\n",
        "        \n",
        "        # Add this sample to the replay buffer after each agent takes a move\n",
        "        if UPDATE_COUNTER == 2:\n",
        "            UPDATE_COUNTER = 0\n",
        "            transition = (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls)\n",
        "            replay_buff.add_sample(transition)\n",
        "\n",
        "            # Clean up after adding\n",
        "            S_sp=S_ls=A_sp=A_ls=R_sp=R_ls=S_next_sp=S_next_ls=done_sp=done_ls=None\n",
        "\n",
        "            # Trains the centralized critic\n",
        "            # critic_dqn.train(done_sp or done_ls)\n",
        "        \n",
        "        # Trains the agent if has enough data\n",
        "        # nash_dqn_agent.train(termination or truncation)\n",
        "\n",
        "    # store the total rewards for last game play in one episode\n",
        "    # for name in AGENT_NAMES:\n",
        "        # ALL_REWARDS[name].append(episode_agent_reward[name]/MAX_CYCLES)\n",
        "\n",
        "    # Peform epsilon decay\n",
        "    if epsilon > MIN_EPSILON:\n",
        "            epsilon *= EPSILON_DECAY\n",
        "            epsilon = max(MIN_EPSILON, epsilon)\n",
        "\n",
        "\n",
        "# # Finally, plot the average reward per step per episode per agent\n",
        "# plt.plot(range(EPISODES), ALL_REWARDS['speaker_0'])\n",
        "# plt.title('Avg reward per step for speaker_0')\n",
        "# plt.xlabel('num_episodes')\n",
        "# plt.ylabel('reward')\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(range(EPISODES), ALL_REWARDS['listener_0'])\n",
        "# plt.title('Avg reward per step for listener_0')\n",
        "# plt.xlabel('num_episodes')\n",
        "# plt.ylabel('reward')\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}