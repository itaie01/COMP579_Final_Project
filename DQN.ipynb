{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhongjie-wu/579project/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V77xL62U1Xg-",
        "outputId": "32bf1fa7-766e-4256-ff17-8cb3ee992992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading PettingZoo-1.22.3-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium>=0.26.0\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from pettingzoo[mpe]) (1.22.4)\n",
            "Collecting pygame==2.1.3.dev8\n",
            "  Downloading pygame-2.1.3.dev8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (6.4.1)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (4.5.0)\n",
            "Collecting jax-jumpy>=1.0.0\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (3.15.0)\n",
            "Installing collected packages: farama-notifications, pygame, jax-jumpy, gymnasium, pettingzoo\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 pettingzoo-1.22.3 pygame-2.1.3.dev8\n"
          ]
        }
      ],
      "source": [
        "! pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QJMRjKpmG47o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "from pettingzoo.mpe import simple_speaker_listener_v3, simple_reference_v2, simple_world_comm_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d3jaeH94HdE8"
      },
      "outputs": [],
      "source": [
        "#Global Variables\n",
        "DISCOUNT = 0.99\n",
        "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
        "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "MODEL_NAME = 'model'\n",
        "MIN_REWARD = -200  # For model save\n",
        "MEMORY_FRACTION = 0.20\n",
        "\n",
        "# Environments\n",
        "EPISODES = 100\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # decaying epsilon\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001\n",
        "\n",
        "#  Stats settings\n",
        "AGGREGATE_STATS_EVERY = 50  # episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VlBuoDBsH2iz"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, input_layer_size, action_space_size):\n",
        "        # Main model which we use to train\n",
        "        self.model = self.create_model(input_layer_size, action_space_size)\n",
        "\n",
        "        # Target network to make sure the updating is stable\n",
        "        self.target_model = self.create_model(input_layer_size, action_space_size)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # The array to keep the memory for the last n steps for training\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "        # Count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "\n",
        "    def create_model(self, input_layer_size, action_space_size):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation='relu', input_shape=(input_layer_size,)))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(action_space_size, activation = 'linear'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    def train(self, terminal_state, step):\n",
        "        # Start training only if enough transition samples has been collected in the memory\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Get a minibatch from memory replay table\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # Get the current states and their corresponding q values for each sample in the minibatch\n",
        "        current_states = np.array([transition[0] for transition in minibatch])\n",
        "        current_qs_list = self.model.predict(current_states)\n",
        "\n",
        "        # Get the next states their corresponding q values for each sample in the minibatch\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
        "        future_qs_list = self.target_model.predict(new_current_states)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "            \n",
        "            # Update Q value for the given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # Prepare training data\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False)\n",
        "\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "        \n",
        "        # update target network with weights of main network if condition satisfied\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xrb3gqkPzx3r"
      },
      "outputs": [],
      "source": [
        "listener_dqn = DQNAgent(input_layer_size=11, action_space_size=5)\n",
        "speaker_dqn = DQNAgent(input_layer_size=3, action_space_size=3)\n",
        "env = simple_speaker_listener_v3.env(max_cycles=200, continuous_actions=False)\n",
        "\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
        "\n",
        "    # Restarting episode and environment\n",
        "    episode_reward = 0\n",
        "    step = 1\n",
        "    env.reset()\n",
        "    current_state_speaker =\n",
        "    current_state_listener = \n",
        "\n",
        "    for agent in env.agent_iter():\n",
        "        if agent == 'speaker_0':\n",
        "            if np.random.random() > epsilon:\n",
        "            # Get action from Q table\n",
        "                action = np.argmax(agent.get_qs(current_state))\n",
        "            else:\n",
        "            # Get random action\n",
        "                action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPgdottxFFRQsRJPK6rsKrT",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "0822a54a5dcf558552277e0e3d2d71b79c858f6dfa2317b682e5eae99f200a1e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
