{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhongjie-wu/579project/blob/main/DQN_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sqSJleTE8jW",
        "outputId": "8183ce7e-817f-459a-9983-3c76392f838b"
      },
      "outputs": [],
      "source": [
        "# !pip install pettingzoo\n",
        "# !pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QJMRjKpmG47o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from pettingzoo.mpe import simple_speaker_listener_v3, simple_reference_v2, simple_world_comm_v2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "from DQNAgent import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LvhGKHMSE2Ha"
      },
      "outputs": [],
      "source": [
        "def eps_greedy_act_selection(epsilon, action_space_size, q_values):\n",
        "    if np.random.random() < epsilon:\n",
        "        # randomly choose one action\n",
        "        return np.random.randint(0, action_space_size)\n",
        "    else:\n",
        "        # all q values\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "def normalization(X):\n",
        "        mu = np.mean(X, axis=0)\n",
        "        sigma = np.std(X, axis=0)\n",
        "    \n",
        "        # Normalize the features using the mean and standard deviation\n",
        "        X_norm = (X - mu) / sigma\n",
        "\n",
        "        return X_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, input_layer_size, action_space_size):\n",
        "        # Main model which we use to train\n",
        "        self.model = self.create_model(input_layer_size, action_space_size)\n",
        "\n",
        "        # Target network to make sure the updating is stable\n",
        "        self.target_model = self.create_model(input_layer_size, action_space_size)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # The array to keep the memory for the last n steps for training\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "        # Count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "\n",
        "    def create_model(self, input_layer_size, action_space_size):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation='relu', input_shape=(input_layer_size,)))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(action_space_size, activation = 'linear'))\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        # transition = (s, a, r, s', done)\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "\n",
        "    def train(self, terminal_state):\n",
        "        # Start training only if enough transition samples has been collected in the memory\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Get a minibatch from memory replay table\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # Get the current states and their corresponding q values for each sample in the minibatch\n",
        "        current_states = np.array([transition[0] for transition in minibatch])\n",
        "        # current_states = StandardScaler().fit_transform(current_states)\n",
        "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
        "\n",
        "        # Get the next states their corresponding q values for each sample in the minibatch\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
        "        # new_current_states = StandardScaler().fit_transform(new_current_states)\n",
        "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "            \n",
        "            # Update Q value for the given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # Prepare training data\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "            # Perform normalization on X\n",
        "            # X = StandardScaler().fit_transform(np.array(X))\n",
        "\n",
        "        self.model.fit(np.array(X), np.array(y), shuffle=False, verbose=0)\n",
        "\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "        \n",
        "        # update target network with weights of main network if condition satisfied\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape), verbose=0)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hVMWkmqXE2Hb"
      },
      "outputs": [],
      "source": [
        "AGENT_NAMES = ['speaker_0', 'listener_0']\n",
        "AGENT_INFOS = {\n",
        "    name:\n",
        "        {\n",
        "            \"state_space_size\": 3 if name == 'speaker_0' else 11, \n",
        "            \"action_space_size\": 3 if name == 'speaker_0' else 5 \n",
        "        } for name in AGENT_NAMES\n",
        "}\n",
        "\n",
        "def dqn_sl(epsilon, num_episode, max_cycles, env):    \n",
        "    # initialize DQNAgents for the listener / speaker\n",
        "    all_dqn_agents = {name: DQNAgent(AGENT_INFOS[name][\"state_space_size\"], \n",
        "                                     AGENT_INFOS[name][\"action_space_size\"]) for name in AGENT_NAMES}\n",
        "    \n",
        "    all_rewards = {agent_name:[] for agent_name in AGENT_NAMES}\n",
        "    \n",
        "    for episode in tqdm(range(1, num_episode + 1), ascii=True, unit='episodes'):\n",
        "    # for episode in range(num_episode):\n",
        "        # initialize environment & reward\n",
        "        env.reset()\n",
        "        episode_agent_reward = {agent_name:0 for agent_name in AGENT_NAMES}\n",
        "        \n",
        "        for agent in env.agent_iter():\n",
        "            # do not do step if terminated \n",
        "            if env.truncations[agent] == True or env.terminations[agent] == True:\n",
        "                    env.step(None)\n",
        "                    continue    \n",
        "            # storing size of action and state (metadata about agent)\n",
        "            agent_info = AGENT_INFOS[agent]\n",
        "            # store the corresponding dqn agent (not the game agent, the agent that does dqn stuff)\n",
        "            dqn_agent = all_dqn_agents[agent]\n",
        "            # actual agent living in environment\n",
        "            game_agent_cur_state = env.observe(agent)\n",
        "            \n",
        "            game_agent_q_val = dqn_agent.get_qs(game_agent_cur_state)\n",
        "            game_agent_action_size = agent_info[\"action_space_size\"]\n",
        "            action_taken = eps_greedy_act_selection(epsilon, game_agent_action_size, game_agent_q_val)\n",
        "            \n",
        "            # take the action choosen from eps greedy\n",
        "            env.step(action_taken)\n",
        "            \n",
        "            # get reward and accumulate it\n",
        "            _, R, termination, truncation, info = env.last()\n",
        "            done = termination or truncation\n",
        "            episode_agent_reward[agent] += R\n",
        "            \n",
        "            # get next state S'\n",
        "            game_agent_next_state = env.observe(agent)\n",
        "            \n",
        "            # update replay memory, and train if we have enough replay memory\n",
        "            dqn_agent.update_replay_memory((game_agent_cur_state, action_taken, R, game_agent_next_state, done))\n",
        "            dqn_agent.train(done)\n",
        "        \n",
        "        # store the total rewards for last game play in one episode\n",
        "        for name in AGENT_NAMES:\n",
        "            all_rewards[name].append(episode_agent_reward[name]/max_cycles)\n",
        "        \n",
        "        if epsilon > MIN_EPSILON:\n",
        "            epsilon *= EPSILON_DECAY\n",
        "            epsilon = max(MIN_EPSILON, epsilon)\n",
        "             \n",
        "    return all_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cgzEwOOgYxzt"
      },
      "outputs": [],
      "source": [
        "# Returns a initialized weight vector with 0\n",
        "def rand_sl(num_episode, max_cycles,  env):\n",
        "    \n",
        "    \n",
        "    all_rewards = {agent_name:[] for agent_name in AGENT_NAMES}\n",
        "    \n",
        "    for i in range(num_episode):\n",
        "        env.reset()\n",
        "        episode_agent_reward = {agent_name:0 for agent_name in AGENT_NAMES}\n",
        "        for agent in env.agent_iter():\n",
        "            if env.truncations[agent] == True or env.terminations[agent] == True:\n",
        "                env.step(None)\n",
        "                continue\n",
        "            \n",
        "            A = env.action_space(agent).sample()\n",
        "\n",
        "            env.step(A)\n",
        "            _, R, termination, truncation, info = env.last()\n",
        "            episode_agent_reward[agent] += R\n",
        "\n",
        "            if termination or truncation:\n",
        "                continue\n",
        "            \n",
        "        # store the total rewards for last game play in one episode\n",
        "        for name in AGENT_NAMES:\n",
        "            all_rewards[name].append(episode_agent_reward[name]/max_cycles)\n",
        "    return all_rewards"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "0822a54a5dcf558552277e0e3d2d71b79c858f6dfa2317b682e5eae99f200a1e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
