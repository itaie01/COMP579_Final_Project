{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pettingzoo[mpe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from pettingzoo.mpe import simple_speaker_listener_v3, simple_reference_v2, simple_world_comm_v2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "DISCOUNT = 0.99\n",
    "MINIBATCH_SIZE = 32  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "\n",
    "# Environments\n",
    "EPISODES = 500\n",
    "MAX_CYCLES = 25\n",
    "\n",
    "REPLAY_MEMORY_SIZE = int(EPISODES * MAX_CYCLES / 5)  # How many last steps to keep for model training\n",
    "CRITIC_MIN_REPLAY_MEMORY_SIZE = int(REPLAY_MEMORY_SIZE / 5 /2)\n",
    "AGENT_MIN_REPLAY_MEMORY_SIZE = CRITIC_MIN_REPLAY_MEMORY_SIZE + 200  # Minimum number of steps in a memory to start training\n",
    "\n",
    "# Exploration settings\n",
    "EPSILON = 0.1  # decaying epsilon\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint_path_sp = \"./models/speaker\"\n",
    "checkpoint_path_ls = \"./models/listener\"\n",
    "checkpoint_path_critic = \"./models/critic\"\n",
    "checkpoint_dir_sp = os.path.dirname(checkpoint_path_sp)\n",
    "checkpoint_dir_ls = os.path.dirname(checkpoint_path_ls)\n",
    "checkpoint_dir_critic = os.path.dirname(checkpoint_path_critic)\n",
    "\n",
    "cp_callback_sp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path_sp, \n",
    "    verbose=0, \n",
    "    save_weights_only=True,\n",
    "    save_freq=50)\n",
    "\n",
    "cp_callback_ls = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path_ls, \n",
    "    verbose=0, \n",
    "    save_weights_only=True,\n",
    "    save_freq=50)\n",
    "\n",
    "cp_callback_critic = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path_critic, \n",
    "    verbose=0, \n",
    "    save_weights_only=True,\n",
    "    save_freq=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why this is implemented as a seperate class is because the data in this memory will be shared across the listener, speaker and centralized DQN network. Hence, saving into one object saves memory at runtime (i.e. no multiple appending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, max_len = REPLAY_MEMORY_SIZE):\n",
    "        # self.replay_memory = deque(maxlen=max_len)\n",
    "        with open('./models/replay_buffer/my_deque.pickle', 'rb') as f:\n",
    "            # use pickle.load to deserialize the deque object from the file\n",
    "            self.replay_memory = pickle.load(f)\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        # sp = speaker, ls = listener, the format of a sample is:\n",
    "        # (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls)\n",
    "        self.replay_memory.append(sample)\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.replay_memory)\n",
    "    \n",
    "    def sample_minibatch(self, minibatch_size = MINIBATCH_SIZE):\n",
    "        return random.sample(self.replay_memory, minibatch_size)\n",
    "    \n",
    "    def get_mem(self):\n",
    "        return self.replay_memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nash DQN agent (For each game env agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NashDQNAgent:\n",
    "    def __init__(self, input_layer_size, action_space_size, ReplayMemoryObject, is_speaker, critic):\n",
    "        # Check if this agent is a speaker, if not then listener\n",
    "        self.is_speaker = is_speaker\n",
    "\n",
    "        # Main model which we use to train\n",
    "        self.model = self.create_model(input_layer_size, action_space_size)\n",
    "\n",
    "        # Target network to make sure the updating is stable\n",
    "        self.target_model = self.create_model(input_layer_size, action_space_size)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # The array to keep the memory for the last n steps for training\n",
    "        self.replay_memory = ReplayMemoryObject\n",
    "\n",
    "        # Count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "        # Add the critic -- a centalized network to give Q values for joint actions by inputing joint observations\n",
    "        self.critic = critic\n",
    "\n",
    "    def create_model(self, input_layer_size, action_space_size):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_shape=(input_layer_size,)))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(action_space_size, activation = 'linear'))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        model.load_weights(checkpoint_dir_sp) if self.is_speaker else model.load_weights(checkpoint_dir_ls)\n",
    "        return model\n",
    "\n",
    "    def train(self, terminal_state):\n",
    "        # Start training only if enough transition samples has been collected in the memory\n",
    "        if self.replay_memory.get_size() < AGENT_MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch from memory replay table\n",
    "        minibatch = self.replay_memory.sample_minibatch(minibatch_size = MINIBATCH_SIZE)\n",
    "\n",
    "        # Get the current states and their corresponding q values for each sample in the minibatch\n",
    "        current_states = np.array([transition[0] for transition in minibatch]) if self.is_speaker else np.array([transition[1] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "        for index, (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls) in enumerate(minibatch):\n",
    "            done = done_sp if self.is_speaker else done_ls\n",
    "            reward = R_sp if self.is_speaker else R_ls\n",
    "            action = A_sp if self.is_speaker else A_ls\n",
    "            current_state = S_sp if self.is_speaker else S_ls\n",
    "\n",
    "            if not done:\n",
    "                # Calculate Nash Q using the centralized network\n",
    "                joint_observation = np.concatenate((S_next_sp, S_next_ls), axis=None)\n",
    "                joint_q_vals = self.critic.get_qs(joint_observation)\n",
    "                nash_q = np.max(joint_q_vals)\n",
    "                # Nash update\n",
    "                new_q = reward + DISCOUNT * nash_q \n",
    "            else:\n",
    "                new_q = reward\n",
    "            \n",
    "            # Update Q value for the given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # Prepare training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        if self.is_speaker:\n",
    "            self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False, verbose=0, callbacks=[cp_callback_sp])\n",
    "        else:\n",
    "            self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False, verbose=0, callbacks=[cp_callback_ls])\n",
    "\n",
    "\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        # update target network with weights of main network if condition satisfied\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape), verbose=0)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic DQN Agent (For providing the Nash Q value of joint states/actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DQN Agent serves as the critic for our Nash DQN algorithm. It takes in the joint states observed by the two agents, and then output an array of Q_valus that corresponds to each combination of agents'action. In a fully collaborative settings, we know that for a given state, the joint actions that lead to the maximal Q value is the nash equilibria move and this maximal Q value is the Nash_Q value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointCritic:\n",
    "    def __init__(self, input_layer_size, action_space_size, ReplayMemoryObject):\n",
    "        # Main model which we use to train\n",
    "        self.model = self.create_model(input_layer_size, action_space_size)\n",
    "\n",
    "        # Target network to make sure the updating is stable\n",
    "        self.target_model = self.create_model(input_layer_size, action_space_size)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # The array to keep the memory for the last n steps for training\n",
    "        self.replay_memory = ReplayMemoryObject\n",
    "\n",
    "        # Count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "\n",
    "    def create_model(self, input_layer_size, action_space_size):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_shape=(input_layer_size,)))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(action_space_size, activation = 'linear'))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        model.load_weights(checkpoint_path_critic)\n",
    "        return model\n",
    "    \n",
    "    def train(self, terminal_state):\n",
    "        # Start training only if enough transition samples has been collected in the memory\n",
    "        if self.replay_memory.get_size() < CRITIC_MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch from memory replay table\n",
    "        minibatch = self.replay_memory.sample_minibatch(minibatch_size = MINIBATCH_SIZE)\n",
    "\n",
    "        # Get the current states and their corresponding q values for each sample in the minibatch\n",
    "        current_states = np.array([np.concatenate((transition[0], transition[1]), axis=None) for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
    "\n",
    "        # Get the next states their corresponding q values for each sample in the minibatch\n",
    "        new_current_states = np.array([np.concatenate((transition[6], transition[7]), axis=None) for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "        for index, (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls) in enumerate(minibatch):\n",
    "            done = done_sp or done_ls\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = R_sp + R_ls + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = R_sp + R_ls\n",
    "            \n",
    "            # Update Q value for the given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            action_idx = np.ravel_multi_index((A_sp, A_ls), dims=(3, 5))\n",
    "            current_qs[action_idx] = new_q\n",
    "\n",
    "            # Prepare training data\n",
    "            X.append(current_states[index])\n",
    "            y.append(current_qs)\n",
    "\n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, shuffle=False, verbose=0, callbacks=[cp_callback_critic])\n",
    "\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        # update target network with weights of main network if condition satisfied\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape), verbose=0)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_act_selection(epsilon, action_space_size, q_values):\n",
    "    if np.random.random() < epsilon:\n",
    "        # randomly choose one action\n",
    "        return np.random.randint(0, action_space_size)\n",
    "    else:\n",
    "        # all q values\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_NAMES = ['speaker_0', 'listener_0']\n",
    "AGENT_INFOS = {name: {\"agent_idx\": 0 if name == 'speaker_0' else 1,\n",
    "                        \"action_space_size\": 3 if name == 'speaker_0' else 5,\n",
    "                        \"input_layer_size\": 3 if name == 'speaker_0' else 11,\n",
    "                        \"is_speaker\": True if name == 'speaker_0' else False\n",
    "                        } for name in AGENT_NAMES}\n",
    "\n",
    "UPDATE_COUNTER = 0\n",
    "# ALL_REWARDS = {agent_name:[] for agent_name in AGENT_NAMES}\n",
    "with open('./models/reward_dict/my_dict.json', 'r') as f:\n",
    "    # use json.load to deserialize the JSON data from the file to a dictionary object\n",
    "    ALL_REWARDS = json.load(f)\n",
    "\n",
    "epsilon = EPSILON\n",
    "\n",
    "# Create a replay buffer\n",
    "replay_buff = ReplayMemory(max_len = REPLAY_MEMORY_SIZE)\n",
    "\n",
    "# Create the critic DQN Agent\n",
    "critic_dqn = JointCritic(input_layer_size=14, action_space_size=15, ReplayMemoryObject=replay_buff)\n",
    "\n",
    "# Create the Nash DQN Agents\n",
    "NASH_DQN_AGENTS = {name: NashDQNAgent(input_layer_size=AGENT_INFOS[name][\"input_layer_size\"],\n",
    "                                      action_space_size=AGENT_INFOS[name][\"action_space_size\"],\n",
    "                                      ReplayMemoryObject=replay_buff,\n",
    "                                      is_speaker=AGENT_INFOS[name][\"is_speaker\"],\n",
    "                                      critic=critic_dqn) for name in AGENT_NAMES}\n",
    "\n",
    "# Create the environment\n",
    "env = simple_speaker_listener_v3.env(max_cycles=MAX_CYCLES, continuous_actions=False)\n",
    "\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "    # Reset the environment and the reward for this new episode\n",
    "    env.reset()\n",
    "    episode_agent_reward = {agent_name:0 for agent_name in AGENT_NAMES}\n",
    "\n",
    "    # Initialize the SARSD for collecting and building dataset later\n",
    "    S_sp=S_ls=A_sp=A_ls=R_sp=R_ls=S_next_sp=S_next_ls=done_sp=done_ls=None\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        if env.truncations[agent] == True or env.terminations[agent] == True:\n",
    "                env.step(None)\n",
    "                continue\n",
    "        \n",
    "        # Get the current agent\n",
    "        nash_dqn_agent = NASH_DQN_AGENTS[agent]\n",
    "        # Observe the current state\n",
    "        state_curr = env.observe(agent)\n",
    "        # Get the Q values for each action of the curr state\n",
    "        q_state_curr = nash_dqn_agent.get_qs(state_curr)\n",
    "        # Choose and take an action\n",
    "        action = eps_greedy_act_selection(epsilon, AGENT_INFOS[agent][\"action_space_size\"], q_state_curr)\n",
    "        env.step(action)\n",
    "        # Get reward and accumulate it\n",
    "        _, reward, termination, truncation, info = env.last()\n",
    "        episode_agent_reward[agent] += reward\n",
    "        # Observe the next state\n",
    "        state_next = env.observe(agent)\n",
    "\n",
    "        # Update the variables\n",
    "        if nash_dqn_agent.is_speaker:\n",
    "            S_sp = state_curr\n",
    "            A_sp = action\n",
    "            R_sp = reward\n",
    "            S_next_sp = state_next\n",
    "            done_sp = termination or truncation\n",
    "        else:\n",
    "            S_ls = state_curr\n",
    "            A_ls = action\n",
    "            R_ls = reward\n",
    "            S_next_ls = state_next\n",
    "            done_ls = termination or truncation\n",
    "\n",
    "        # Increase the update counter\n",
    "        UPDATE_COUNTER += 1\n",
    "        \n",
    "        # Add this sample to the replay buffer after each agent takes a move\n",
    "        if UPDATE_COUNTER == 2:\n",
    "            UPDATE_COUNTER = 0\n",
    "            transition = (S_sp, S_ls, A_sp, A_ls, R_sp, R_ls, S_next_sp, S_next_ls, done_sp, done_ls)\n",
    "            replay_buff.add_sample(transition)\n",
    "\n",
    "            # Clean up after adding\n",
    "            S_sp=S_ls=A_sp=A_ls=R_sp=R_ls=S_next_sp=S_next_ls=done_sp=done_ls=None\n",
    "\n",
    "            # Trains the centralized critic\n",
    "            critic_dqn.train(done_sp or done_ls)\n",
    "        \n",
    "        # Trains the agent if has enough data\n",
    "        nash_dqn_agent.train(termination or truncation)\n",
    "\n",
    "    # store the total rewards for last game play in one episode\n",
    "    for name in AGENT_NAMES:\n",
    "        ALL_REWARDS[name].append(episode_agent_reward[name]/MAX_CYCLES)\n",
    "\n",
    "    # Peform epsilon decay\n",
    "    if epsilon > MIN_EPSILON:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "            epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "\n",
    "# Finally, plot the average reward per step per episode per agent\n",
    "# plt.plot(range(EPISODES), ALL_REWARDS['speaker_0'])\n",
    "# plt.title('Avg reward per step for speaker_0')\n",
    "# plt.xlabel('num_episodes')\n",
    "# plt.ylabel('reward')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(range(EPISODES), ALL_REWARDS['listener_0'])\n",
    "# plt.title('Avg reward per step for listener_0')\n",
    "# plt.xlabel('num_episodes')\n",
    "# plt.ylabel('reward')\n",
    "# plt.show()\n",
    "\n",
    "# Save the replay buffer\n",
    "with open('./models/replay_buffer/my_deque.pickle', 'wb') as f:\n",
    "    pickle.dump(replay_buff.replay_memory, f)\n",
    "\n",
    "# Save the current reward dictionary\n",
    "with open('./models/reward_dict/my_dict.json', 'w') as f:\n",
    "    # use json.dump to serialize the dictionary object to JSON and write it to the file\n",
    "    json.dump(ALL_REWARDS, f)\n",
    "\n",
    "os.system(\"tmux wait-for -S script_finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
